# Title  
Self-Supervised Learning for Solar Radio Spectrum Classification  

# Author  
Siqi Li et al.  

# Background  
Solar radio bursts provide crucial insights into solar activity and space weather phenomena. Traditional deep learning methods for their classification require extensive labeled training data, often addressed via transfer learning. However, the disparity between natural images and solar radio spectra limits transfer learning effectiveness. Solar broadband radio spectrometers (SBRS) at the National Astronomical Observatories of the Chinese Academy of Sciences generate vast observational data, but the rarity and noise contamination of solar radio bursts complicate automated classification.  

# Motivation  
The surge in solar radio spectral data necessitates automated detection and classification for efficient solar physics research and space weather forecasting. Existing deep learning approaches face challenges due to data scarcity and the domain gap between natural and solar images. Self-supervised learning (SSL), inspired by methods like BERT in natural language processing, offers a solution by leveraging unlabeled data to learn essential features, reducing reliance on labeled datasets and improving transfer learning applicability.  

# Methodology  
The proposed self-supervised learning method adapts self-masking from BERT to solar radio spectrum classification. Solar radio spectrum images are divided into blocks, randomly masked, and processed by a Vision Transformer (ViT) network comprising an encoder and decoder. The encoder processes unmasked blocks, while the decoder reconstructs the image. Preprocessing includes frequency channel normalization using the formula:  
\[
p^{\prime}(x, y)=p(x, y)-\frac{1}{n} \sum_{y=0}^{n} p(x, y)+\frac{1}{m n} \sum_{x=0}^{m} \sum_{y=0}^{n} p(x, y)
\]  
to reduce transverse stripe noise. Data augmentation techniques like mixup, which linearly interpolates images and labels with a Beta-distributed parameter \(\lambda\), and dropout methods (DropPath, DropAttention) are employed to mitigate overfitting. Experiments used a dataset of 5519 samples (4415 training, 1104 testing) with polarization-separated classes. Training configurations included pre-training for 300 epochs (learning rate \(1.5 \times 10^{-4}\)) and fine-tuning for 50 epochs (learning rate \(10^{-3}\)) on an NVidia GeForce RTX 2080Ti GPU.  

# Results  
A masking rate of 75% yielded optimal accuracy, contrasting with BERT’s 15%, due to higher redundancy in images. Transfer learning from ImageNet improved accuracy from 70.60% to 98.63%. Mixup and cutmix augmentation achieved 99.3% accuracy, while DropPath regularization (0.1 drop probability) achieved 98.9% accuracy. The final model achieved 99.5% accuracy, 99.5% precision, 98.9% recall, and 99.7% specificity for burst classification, outperforming Vision Transformer, ResNet, and other benchmarks.  

# Interpretation  
High masking rates enhance feature learning by removing redundant and irrelevant data, enabling the model to focus on discriminative features. Self-supervised pre-training extracts richer image information than supervised methods, improving transfer learning efficacy. Mixup and cutmix augmentations improve generalization by simulating diverse training samples, while DropPath regularization prevents overfitting by randomly dropping network paths.  

# Implication  
This method addresses the scarcity of labeled solar radio spectra and demonstrates SSL’s potential for small-sample astronomical data classification. Future work will optimize model scale for reduced training time and extend the approach to fine-grained classification of solar radio burst types (I, II, III, IV, V), advancing the understanding of solar physical phenomena and space weather forecasting.